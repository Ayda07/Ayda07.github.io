<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>Sources</title>
        <link rel="stylesheet" href="style.css">
    </head>
    <body style="background-image: url('Images/Machine_Learning.gif')">
        <div class="navbar">
            <div id="head" style="text-align: center">
                <h1 style="text-align: center">Bias and Discrimination From Algorithms</h1>
            </div>
            <a href="index.html">Home</a>
            <a href="criminal_justice.html">Criminal Justice</a>
            <a href="social.html">Society</a>
            <a href="sources.html">Sources</a>
        </div>
        <div class="main" style="display: block">
            <div class="textbox center-float" style="float: left">
                <h2>
                    Negative Impacts
                </h2>
                <div class="section">
                    <h3>
                        Statistical Fallacies
                    </h3>
                    <p>
                        Algorithms are often presented as a solution to biased people. They just do math, how could they
                        discriminate? This view is flawed because it does not take into account the source of the data
                        these systems operate on. Data is not pulled from thin air and most systems "are unlikely to
                        gather and record all available data; someone decides what to record and how to record it"
                        (Levmore, 373). This is where bias seeps in. Every person in society has some form of implicit
                        biases, so when the data chosen for a system is not carefully analysed and selected it presents
                        biased trends which become entrenched in the system's virtual reasoning.
                    </p>
                    <p>
                        Bias can also present itself in algorithms through a misunderstanding of statistics. The more
                        prevalent a certain variable is in the data the more important it becomes to the system, so
                        simply taking a system's results at face value is inaccurate. For example, historically minority
                        neighborhoods have been policed much more aggressively than their counterparts. The increase in
                        police presence results in more crime data from those neighborhoods. This disparity in crime
                        data then increases societal "suspicion shaped in part by the number of prior contacts. Even
                        pure algorithmic suspicion, which should avoid charges of racial bias, can be found to be
                        infected with biases," (Ferguson, 132-133). The system will then seem to suggest increasing
                        police activity in these areas reinforcing historical discrimination.
                    </p>
                </div>
                <div class="section">
                    <h3>
                        Presumptive Punishment
                    </h3>
                    <p>
                        The foundation of using algorithms in criminal justice is making court decisions based on the
                        groups a dependent belongs to. It was borderline discrimination from the beginning. Certain
                        applications are safer, such as recidivism prediction, but others are much more dangerous, chiefly
                        algorithmic sentencing. Determining or even suggesting a criminal's sentence based upon groupings
                        is extremely damaging, especially with historic data trending against minorities.
                    </p>
                </div>
            </div>
        </div>
    </body>
</html>