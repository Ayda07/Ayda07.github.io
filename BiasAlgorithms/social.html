<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>Sources</title>
        <link rel="stylesheet" href="style.css">
    </head>
    <body style="background-image: url('Images/Machine_Learning.gif')">
        <div class="navbar">
            <div id="head" style="text-align: center">
                <h1 style="text-align: center">Bias and Discrimination From Algorithms</h1>
            </div>
            <a href="index.html">Home</a>
            <a href="criminal_justice.html">Criminal Justice</a>
            <a href="social.html">Society</a>
            <a href="sources.html">Sources</a>
        </div>
        <div class="main" style="display: block">
            <div class="textbox center-float">
                <h2>
                    Public Influence
                </h2>
                <div class="section">
                    <p>
                        "involves unseen, categorical, computational judgments about which
                        searches, articles, or purchases should probably come next. Users are not
                        offered limitless options but are, in fact, given a narrowly construed set that
                        comes from successfully fitting other people, past actions, and inanimate
                        objects into categories - using categories to discipline action," (Ananny, 103)
                    </p>
                </div>
            </div>
            <div class="textbox center-float">
                <h2>
                    Opportunity Screening
                </h2>
                <div class="section">
                    <div style="float: right; margin-left: 20px; margin-bottom: 20px">
                        <img src="Images/RaceBlindGraph.png" title="(Kleinberg, 159)" alt="Figure 2 (Kleiberg, 159)"/>

                    </div>
                    <p>
                        Machine learning algorithms are often used in systems that screen applicants to jobs, schools,
                        and programs. Due to the types of data used to rank these applicants race or other social
                        groupings are often included implicitly. As shown in the chart to the right algorithms designed
                        to be race blind, will actually negatively effect those they try to benefit, in this case black
                        students. This error is caused "because the relationships between [the data,] high school
                        predictors and [the outcome,] college success turn out differently for [different groups,] white
                        and black students," (Kleinberg, 159). Carefully considering what data is provided to a system is
                        extremely necessary to avoiding a biased algorithm as even seemingly innocuous data can reflect
                        discriminatory social trends.
                    </p>
                    <p>
                        To prevent the propagation of discriminatory practices we must ensure "human scrutiny of the
                        results of algorithms used to make life-changing decisions," (Gumbus, 123).
                        Unfortunately machine learning algorithms inherently conceal how the
                        provided data is used. This black box effect makes it much more difficult to detect weather or
                        not a decision was based on biased observations. You can't know whether you were denied
                        certain opportunities such as a job or scholarship due to something outside your control.
                        Without carefully and openly analysing the implications of the data given to the system we cannot
                        recognize the discrimination perpetuated by flawed systems.
                    </p>
                </div>
            </div>
        </div>
    </body>
</html>